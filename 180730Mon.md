colab에서는 패키지 설치를 노트북마다 설치해야 하는데 
jupyter에선 로컬에 1번 설치하면 계속 쓸 수 있음

개선법) 벡터화 옵션, 랜덤 포레스트 옵션 바꾸기, xgboot 써보기
depth값이 커지면 overfitting 되는 편

국민청원 ===================================
랜덤 포레스트 max_depth = 20
17.043478260869566

랜덤 포레스트 max_depth = 10
17.91304347826087

랜덤 포레스트 max_depth = 12
17.565217391304348

랜덤 포레스트 max_depth = 10,
17.565217391304348

랜덤 포레스트 max_features = "log2"
단어 벡터화 max_features 2000->4000
=> 영향 없음

단어 벡터화 max_df = 0.1
최종 20.8xxx

===================================

<오후>

tensorflow, keras가 더 성능이 좋음
n_jobs = -1 : 코어를 몇 개 쓰게 할 지
mean =0.5 : 추천이 반반
좋은 스테머를 사용하면 예측도를 높일 수 있음!

kaggle =====================================
max_df = 0.1
max_depth = 10 X
n_gram = (1,5) -> 0.9262

단어 모수가 적기 때문에 word2vec이 훨씬 성능이 낮을 때가 있음.
Part. 3는 clustering하여 평균으로 벡터 만들기
TFIDF, xgboost

개선법) 스테밍 바꾸기, 다른 수의 어휘 단어 선택 , 리뷰를 다르게 정리

가정) 부정사 + 부정사, 비꼬아서 쓰기, 반어법, 불용어

1) 스테밍 바꿔보기 : 별 영향 없음
2) 불용어 엄청 추가
3) 랜덤 포레스트 변경
n_estimate = 0.2 
max_df = 0.1
랜덤 포레스트 Ngram (1,5)